{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0fea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56578f6f",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761f4d2",
   "metadata": {},
   "source": [
    "Loading the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to edit the path, depending on where you put the files.\n",
    "data = pd.read_csv(\"data/a4_synthetic.csv\")\n",
    "\n",
    "X = data.drop(columns=\"y\").to_numpy()\n",
    "Y = data.y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166d6ba",
   "metadata": {},
   "source": [
    "Training a linear regression model for this synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111ff34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999661130823178\n",
      "Epoch 2: MSE = 0.017392390107906875\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438456\n",
      "Epoch 5: MSE = 0.009365440968904256\n",
      "Epoch 6: MSE = 0.009366989180952533\n",
      "Epoch 7: MSE = 0.009367207398577986\n",
      "Epoch 8: MSE = 0.009367238983974489\n",
      "Epoch 9: MSE = 0.009367243704122532\n",
      "Epoch 10: MSE = 0.009367244427185763\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# We just declare the parameter tensors. Do not use nn.Linear.\n",
    "w = torch.tensor(w_init, requires_grad=True)\n",
    "b = torch.tensor(b_init, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = torch.optim.SGD([w, b], lr=eta)\n",
    "\n",
    "for i in range(10):\n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        x = torch.tensor(X[[row], :])\n",
    "        y = torch.tensor(Y[[row]])\n",
    "\n",
    "        # Forward pass\n",
    "        opt.zero_grad()\n",
    "        y_pred = x @ w + b\n",
    "        err = (y_pred - y) ** 2\n",
    "\n",
    "        # Backward and update.\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f\"Epoch {i+1}: MSE =\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f221d",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56be71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "\n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = \", requires_grad=True\"\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f\", grad_fn={self.grad_fn}\"\n",
    "        else:\n",
    "            gstr = \"\"\n",
    "        return f\"Tensor({dstr}{gstr})\"\n",
    "\n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()\n",
    "\n",
    "    # For Task 2:\n",
    "\n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        # performs add operation\n",
    "        new_data = self.data + right.data\n",
    "        grad_fn = AdditionNode(self, right)\n",
    "        if self.requires_grad or right.requires_grad:\n",
    "            return Tensor(new_data, grad_fn=grad_fn, requires_grad=True)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        # performs subtraction operation\n",
    "        new_data = self.data - right.data\n",
    "        grad_fn = SubtractionNode(self, right)\n",
    "        if self.requires_grad or right.requires_grad:\n",
    "            return Tensor(new_data, grad_fn=grad_fn, requires_grad=True)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        # performs matrix multiplication operation\n",
    "        new_data = self.data @ right.data\n",
    "        grad_fn = MatMulNode(self, right)\n",
    "        if self.requires_grad or right.requires_grad:\n",
    "            return Tensor(new_data, grad_fn=grad_fn, requires_grad=True)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # performs power operation\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception(\"only integers allowed\")\n",
    "        if right < 2:\n",
    "            raise Exception(\"power must be ∏= 2\")\n",
    "        grad_fn = PowNode(self, right)\n",
    "        new_data = self.data**right\n",
    "        if self.requires_grad:\n",
    "            return Tensor(new_data, grad_fn=grad_fn, requires_grad=True)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "    def tanh(self):\n",
    "        # performs tanh operation\n",
    "        new_data = np.tanh(self.data)\n",
    "        grad_fn = TanhNode(self)\n",
    "        if self.requires_grad:\n",
    "            return Tensor(new_data, grad_fn=grad_fn, requires_grad=True)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        # performs sigmoid operation\n",
    "        new_data = sigmoid(self.data)\n",
    "        grad_fn = SigmoidNode(self)\n",
    "        if self.requires_grad:\n",
    "            return Tensor(new_data, grad_fn=grad_fn, requires_grad=True)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "    # Backward computations. Will be implemented in Task 4.\n",
    "    def backward(self, grad_output=None):\n",
    "        # We first check if this tensor has a grad_fn: that is, one of the\n",
    "        # nodes that you defined in Taskprint(\"called backward\")\n",
    "        if self.grad_fn is not None:\n",
    "            # If grad_fn is defined, we have computed this tensor using some operation.\n",
    "            if grad_output is None:\n",
    "                # This is the starting point of the backward computation.\n",
    "                # This will typically be the tensor storing the output of\n",
    "                # the loss function, on which we have called .backward()\n",
    "                # in the training loop.\n",
    "\n",
    "                # We always have a gradient of 1.0 with respect to the output of the loss at the start.\n",
    "                self.grad_fn.backward(1)\n",
    "\n",
    "            else:\n",
    "                # This is an intermediate node in the computational graph.\n",
    "                # This corresponds to any intermediate computation, such as\n",
    "                \n",
    "                # Here we simply pass the current gradient for this tensor as the output gradient for the next backward step\n",
    "                self.grad_fn.backward(self.grad)\n",
    "\n",
    "        else:\n",
    "            # If grad_fn is not defined, this is an endpoint in the computational\n",
    "            # graph: learnable model parameters or input data.\n",
    "\n",
    "            if self.requires_grad:\n",
    "                # This tensor *requires* a gradient to be computed. This will\n",
    "                # typically be a tensor that holds learnable parameters.\n",
    "\n",
    "                # The resulting gradient is simply the output gradient\n",
    "                \n",
    "                self.grad = grad_output\n",
    "            else:\n",
    "                # This tensor *does not require* a gradient to be computed. This\n",
    "                # will typically be a tensor holding input data.\n",
    "            \n",
    "                self.grad = None\n",
    "\n",
    "\n",
    "# A small utility where we simply create a Tensor object. We use this to\n",
    "# mimic torch.tensor.\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Helper function to compute the sigmoid.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0f04c",
   "metadata": {},
   "source": [
    "Some sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2014827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2**2\n",
    "test_matmul = x1 @ w\n",
    "test_combination = (x1**2 - x2 @ w) ** 3\n",
    "\n",
    "print(f\"Test of addition: {x1.data} + {x2.data} = {test_plus.data}\")\n",
    "print(f\"Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}\")\n",
    "print(f\"Test of power: {x2.data} ** 2 = {test_power.data}\")\n",
    "print(f\"Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}\")\n",
    "\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert np.allclose(test_plus.data, np.array([[3.0, 7.0]]))\n",
    "assert np.allclose(test_minus.data, np.array([[1.0, -1.0]]))\n",
    "assert np.allclose(test_power.data, np.array([[1.0, 16.0]]))\n",
    "assert np.allclose(test_matmul.data, np.array([[1.6]]))\n",
    "assert np.allclose(test_combination.data, np.array([[8.00000e-03, 1.40608e02]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c645c32",
   "metadata": {},
   "source": [
    "# Tasks 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9133db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError(\"Unimplemented\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(type(self))\n",
    "\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right \n",
    "        \n",
    "\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        # result = left + right\n",
    "        # d_L/d_left = d_L/d_result * d_result/d_left = grad_output * 1\n",
    "        # d_L/d_right = d_L/d_result * d_result/d_right = grad_output * 1\n",
    "\n",
    "        self.left.grad = grad_output\n",
    "        self.right.grad = grad_output\n",
    "        self.right.backward(self.right.grad)\n",
    "        self.left.backward(self.left.grad)\n",
    "\n",
    "\n",
    "class SubtractionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # result = left - right\n",
    "        # d_L/d_left = d_L/d_result * d_result/d_left = grad_output * 1\n",
    "        # d_L/d_right = d_L/d_result * d_result/d_right = grad_output * -1\n",
    "\n",
    "        self.left.grad = grad_output\n",
    "        self.right.grad = -grad_output\n",
    "        self.right.backward(self.right.grad)\n",
    "        self.left.backward(self.left.grad)\n",
    "\n",
    "\n",
    "class MatMulNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # result = left @ right\n",
    "        # d_L/d_left = d_L/d_result * d_result/d_left = grad_output @ right.T\n",
    "        # d_L/d_right = d_L/d_result * d_result/d_right = left.T @ grad_output\n",
    "        \n",
    "        \n",
    "        self.left.grad =  grad_output @ self.right.data.T\n",
    "        self.right.grad = self.left.data.T @ grad_output\n",
    "            \n",
    "        self.right.backward(self.right.grad)\n",
    "        self.left.backward(self.left.grad)\n",
    "\n",
    "\n",
    "class PowNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # result = left ** right\n",
    "        # d_L/d_left = d_L/d_result * d_result/d_left = grad_output * right * left ** (right - 1)\n",
    "\n",
    "        self.left.grad = (self.right * self.left.data ** (self.right - 1)) * grad_output\n",
    "        self.left.backward(self.left.grad)\n",
    "        \n",
    "        \n",
    "class TanhNode(Node):\n",
    "    def __init__(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # result = tanh(left)\n",
    "        # d_L/d_left = d_L/d_result * d_result/d_left = grad_output * (1 - tanh(left) ** 2)\n",
    "        \n",
    "        self.left.grad = (1 - (np.tanh(self.left.data) ** 2)) * grad_output\n",
    "        self.left.backward(self.left.grad)\n",
    "        \n",
    "class SigmoidNode(Node):\n",
    "    def __init__(self, left):\n",
    "        self.left = left\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # result = sigmoid(left)\n",
    "        # d_L/d_left = d_L/d_result * d_result/d_left = grad_output * (sigmoid(left) * (1 - sigmoid(left)))\n",
    "        \n",
    "        self.left.grad = grad_output * (sigmoid(self.left.data) * (1 - sigmoid(self.left.data)))\n",
    "        self.left.backward(self.left.grad)\n",
    "        \n",
    "class BCELossNode(Node):\n",
    "    def __init__(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "\n",
    "    def forward(self):\n",
    "        # forward for BCEloss \n",
    "        self.output = -np.mean(self.y_true.data * np.log(self.y_pred.data) + (1 - self.y_true.data) * np.log(1 - self.y_pred.data))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # result = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        # d_L/d_y_pred = d_L/d_result * d_result/d_y_pred = grad_output * (-y_true / y_pred + (1 - y_true) / (1 - y_pred)) / num_elements\n",
    "        \n",
    "        num_elements = np.prod(self.y_pred.data.shape)\n",
    "        grad_y_pred = grad_output * (-self.y_true.data / self.y_pred.data + (1 - self.y_true.data) / (1 - self.y_pred.data)) / num_elements\n",
    "        self.y_pred.grad = grad_y_pred\n",
    "        self.y_pred.backward(self.y_pred.grad)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1bb77-e869-4e08-8996-3674eed101e6",
   "metadata": {},
   "source": [
    "Sanity check for Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3276aba-4def-421b-b12e-bf0d7120f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print(\"Computational graph top node after x + w1 + w2:\", test_graph.grad_fn)\n",
    "\n",
    "assert isinstance(test_graph.grad_fn, AdditionNode)\n",
    "assert test_graph.grad_fn.right is w2\n",
    "assert test_graph.grad_fn.left.grad_fn.left is x\n",
    "assert test_graph.grad_fn.left.grad_fn.right is w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a9bfb-ea55-4bce-9356-4956316e1904",
   "metadata": {},
   "source": [
    "Sanity check for Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32687661-a67d-4bef-9a90-7dabb93380a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w\n",
    "diff = model_out - y\n",
    "loss = diff**2\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient of loss w.r.t. w =\\n\", w.grad)\n",
    "\n",
    "assert np.allclose(w.grad, np.array([[5.6], [8.4]]))\n",
    "assert x.grad is None\n",
    "assert y.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cc295",
   "metadata": {},
   "source": [
    "An equivalent cell using PyTorch code. Your implementation should give the same result for `w.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabcc94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6000],\n",
       "        [8.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n",
    "pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "pt_y = torch.tensor(np.array([[0.2]]))\n",
    "\n",
    "pt_model_out = pt_x @ pt_w\n",
    "pt_model_out.retain_grad()  # Keep the gradient of intermediate nodes for debugging.\n",
    "\n",
    "pt_diff = pt_model_out - pt_y\n",
    "pt_diff.retain_grad()\n",
    "\n",
    "pt_loss = pt_diff**2\n",
    "pt_loss.retain_grad()\n",
    "\n",
    "pt_loss.backward()\n",
    "pt_w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5439b",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b03a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError(\"Unimplemented\")\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        # Update the parameters in the negative gradient direction of the gradient. (minimize the loss function)\n",
    "        for p in self.params:\n",
    "            if p.requires_grad:\n",
    "                p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685a8207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999661130823178\n",
      "Epoch 2: MSE = 0.017392390107906875\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438456\n",
      "Epoch 5: MSE = 0.009365440968904256\n",
      "Epoch 6: MSE = 0.009366989180952533\n",
      "Epoch 7: MSE = 0.009367207398577986\n",
      "Epoch 8: MSE = 0.009367238983974489\n",
      "Epoch 9: MSE = 0.009367243704122532\n",
      "Epoch 10: MSE = 0.009367244427185763\n"
     ]
    }
   ],
   "source": [
    "# You may need to edit the path, depending on where you put the files.\n",
    "np.random.seed(1)\n",
    "data = pd.read_csv(\"data/a4_synthetic.csv\")\n",
    "\n",
    "X = data.drop(columns=\"y\").to_numpy()\n",
    "Y = data.y.to_numpy()\n",
    "\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# We just declare the parameter tensors. Do not use nn.Linear.\n",
    "w = tensor(w_init, requires_grad=True)\n",
    "b = tensor(b_init, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = SGD([w, b], lr=eta)\n",
    "\n",
    "for i in range(10):\n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        x = tensor(X[[row], :])\n",
    "        y = tensor(Y[[row]])\n",
    "\n",
    "        # Forward pass\n",
    "        opt.zero_grad()\n",
    "        y_pred = x @ w + b\n",
    "        err = (y_pred - y) ** 2\n",
    "\n",
    "        # Backward and update.\n",
    "        # TODO: compute gradients and then update the model.\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f\"Epoch {i+1}: MSE =\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bef171",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da62980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You may need to edit the path, depending on where you put the files.\n",
    "a4data = pd.read_csv(\"data/raisins.csv\")\n",
    "\n",
    "X = scale(a4data.drop(columns=\"Class\"))\n",
    "Y = 1.0 * (a4data.Class == \"Besni\").to_numpy()\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(Y))\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a37229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Module:\n",
    "    # Create a basic module to zero the gradients of all parameters.\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def parameters(self):\n",
    "        raise NotImplementedError(\"Method for getting parameters is not implemented\")\n",
    "\n",
    "class Layer(Module):\n",
    "    # A simple dense layer with a tanh or sigmoid activation function., creates a weight and bias tensor\n",
    "    def __init__(self, in_features, out_features, activation: str = \"tanh\"):\n",
    "        self.w = tensor(np.random.normal(size=(in_features, out_features)), requires_grad=True)\n",
    "        self.b = tensor(np.random.normal(size=(1, out_features)), requires_grad=True)\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        model_output = x @ self.w + self.b\n",
    "\n",
    "        if self.activation == \"tanh\":\n",
    "            return model_output.tanh()\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return model_output.sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {self.activation}\")\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "\n",
    "class MLP(Module):\n",
    "    # A simple multi-layer perceptron with one hidden layer.\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        self.hidden = Layer(in_features, hidden_features)\n",
    "        self.out = Layer(hidden_features, out_features, activation=\"sigmoid\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.out(self.hidden(x))\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in [self.hidden, self.out] for p in layer.parameters()]\n",
    "\n",
    "    def disable_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "def bce_loss(y_pred, y_true):\n",
    "    # Compute the binary cross-entropy loss.\n",
    "    return Tensor(BCELossNode(y_pred, y_true).forward(), grad_fn=BCELossNode(y_pred, y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f122b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_vs_torch():\n",
    "    \"\"\"Simple test to compare the implementation for sigmoid, tanh and bceloss with PyTorch.\"\"\"\n",
    "    bce = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "    w_init = np.random.normal(size=(2, 1))\n",
    "    x_init = np.random.normal(size=(1, 2))\n",
    "    w_torch = torch.tensor(w_init, requires_grad=True)\n",
    "    x_torch = torch.tensor(x_init, requires_grad=False)\n",
    "    w_tensor = tensor(w_init, requires_grad=True)\n",
    "    x_tensor = tensor(x_init, requires_grad=False)\n",
    "    \n",
    "    out_torch = x_torch @ w_torch\n",
    "    out_torch = out_torch.sigmoid()\n",
    "    err_torch = bce(out_torch, torch.tensor([[1.0]], dtype=torch.float64))\n",
    "    err_torch.backward()\n",
    "    \n",
    "    out_tensor = x_tensor @ w_tensor\n",
    "    out_tensor = out_tensor.sigmoid()\n",
    "    err_tensor = bce_loss(out_tensor, tensor(np.array([[1.0]])))\n",
    "    err_tensor.backward()\n",
    "    \n",
    "    \n",
    "    assert np.allclose(w_tensor.grad, w_torch.grad.numpy()), \"Gradients do not match for BCE loss\"\n",
    "    assert np.allclose(out_tensor.data, out_torch.data.numpy()), \"Output does not match for sigmoid\"\n",
    "    \n",
    "    \n",
    "    w_torch = torch.tensor(w_init, requires_grad=True)\n",
    "    x_torch = torch.tensor(x_init, requires_grad=False)\n",
    "    w_tensor = tensor(w_init, requires_grad=True)\n",
    "    x_tensor = tensor(x_init, requires_grad=False)\n",
    "\n",
    "    out_torch = x_torch @ w_torch\n",
    "    out_torch = out_torch.tanh()\n",
    "    \n",
    "    out_tensor = x_tensor @ w_tensor\n",
    "    out_tensor = out_tensor.tanh()\n",
    "    \n",
    "    assert np.allclose(out_tensor.data, out_torch.data.numpy()), \"Output does not match for tanh\"\n",
    "    \n",
    "\n",
    "test_vs_torch()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4adf76cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: BCE = 0.6234923395239745\n",
      "Epoch 2: BCE = 0.4014429960012952\n",
      "Epoch 3: BCE = 0.3636728892030264\n",
      "Epoch 4: BCE = 0.33976173890169425\n",
      "Epoch 5: BCE = 0.32221372001882365\n",
      "Epoch 6: BCE = 0.3098793292138743\n",
      "Epoch 7: BCE = 0.30177117733037573\n",
      "Epoch 8: BCE = 0.29620560115160516\n",
      "Epoch 9: BCE = 0.2921147371002634\n",
      "Epoch 10: BCE = 0.2889261433814792\n",
      "Accuracy on test set: 85.56%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "model = MLP(Xtrain.shape[1], 100, 1)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, X, Y, epochs=10, lr=0.01):\n",
    "    opt = SGD(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        sum_err = 0\n",
    "        for row in range(X.shape[0]):\n",
    "            x = tensor(X[[row], :])\n",
    "            y = tensor(Y[[row]])\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            err = bce_loss(y_pred, y)\n",
    "            err.backward()\n",
    "            opt.step()\n",
    "            sum_err += err.item()\n",
    "        bce = sum_err / X.shape[0]\n",
    "        print(f\"Epoch {epoch+1}: BCE =\", bce)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, test_labels):\n",
    "    model.disable_grad()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "   \n",
    "    for x, y in zip(test_data, test_labels):\n",
    "        inputs = tensor(x)\n",
    "        label = tensor(y)\n",
    "        outputs = model(inputs)\n",
    "        predicted = np.round(outputs.item())\n",
    "        total += 1\n",
    "        correct += (predicted == label.item())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy on test set: {accuracy:.2%}\")\n",
    "    \n",
    "    \n",
    "train_model(model, Xtrain, Ytrain, epochs=10)    \n",
    "\n",
    "evaluate_model(model, Xtest, Ytest)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
